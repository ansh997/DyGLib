{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "\n",
    "# scratch_location = r'/scratch/hmnshpl'\n",
    "import os\n",
    "import sys\n",
    "import heapq\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_name = 'reddit'\n",
    "scratch_location = rf'/scratch/{getpass.getuser()}'\n",
    "\n",
    "\n",
    "## Load Data\n",
    "# Load data and train val test split\n",
    "graph_df = pd.read_csv('{}/processed_data/{}/ml_{}.csv'.format(scratch_location,\n",
    "                                                            dataset_name,\n",
    "                                                            dataset_name)\n",
    "                    )\n",
    "edge_raw_features = np.load('{}/processed_data/{}/ml_{}.npy'.format(scratch_location,\n",
    "                                                                    dataset_name,\n",
    "                                                                    dataset_name)\n",
    "                            )\n",
    "node_raw_features = np.load('{}/processed_data/{}/ml_{}_node.npy'.format(scratch_location,\n",
    "                                                                        dataset_name,\n",
    "                                                                        dataset_name)\n",
    "                            )\n",
    "\n",
    "# Set the working directory to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')) # this might cause issue\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_data.temporal_pr import temporal_pagerank_with_timestamps, calc_timestamp_pagerank,\\\n",
    "    calc_inc_timestamp_pagerank, optimized_calc_inc_timestamp_pagerank,\\\n",
    "    get_temporal_pagerank, mean_shift_removal, mean_shift_removal2, compute_mean_shifts_with_metrics, \\\n",
    "    calculate_temporal_edge_rank, calculate_combined_temporal_edgerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15 0.15\n",
      "1882469.2648 2261813.6575999996\n"
     ]
    }
   ],
   "source": [
    "# get the timestamp of validate and test set\n",
    "val_ratio = test_ratio = 0.15\n",
    "print(val_ratio, test_ratio)\n",
    "val_time, test_time = list(np.quantile(graph_df.ts, [(1 - val_ratio - test_ratio), (1 - test_ratio)]))\n",
    "print(val_time, test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>ts</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10002</td>\n",
       "      <td>6.320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10003</td>\n",
       "      <td>7.026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10003</td>\n",
       "      <td>13.599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10004</td>\n",
       "      <td>16.811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  u      i      ts  label  idx\n",
       "0           0  1  10001   0.000    0.0    1\n",
       "1           1  2  10002   6.320    0.0    2\n",
       "2           2  3  10003   7.026    0.0    3\n",
       "3           3  4  10003  13.599    0.0    4\n",
       "4           4  5  10004  16.811    0.0    5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph_df = graph_df[graph_df['ts'] < val_time]\n",
    "train_graph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470713\n",
      "0.7000001487106047\n"
     ]
    }
   ],
   "source": [
    "print(len(train_graph_df))\n",
    "print(len(train_graph_df) / len(graph_df))  # is 70% of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "upto=0.7\n",
    "metric='chebyshev'\n",
    "# wasserstein, kl_divergence\n",
    "# jensen_shannon_divergence -- 1m47s\n",
    "# wasserstein -- 3m31s\n",
    "# kl_divergence -- 32s\n",
    "\n",
    "\n",
    "# tmp_graph = tmp_graph.sort_values(by=['u', 'i', 'ts'])\n",
    "\n",
    "# # Exclude the first and last rows based on 'u' and 'i'\n",
    "# grouped = tmp_graph.groupby(['u', 'i'])\n",
    "# modified_df = grouped.apply(lambda x: x.iloc[1:-1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_groups = len(grouped)\n",
    "# rows_removed = total_groups * 2 \n",
    "# total_rows_original = len(tmp_graph)\n",
    "# percentage_removed = (rows_removed / total_rows_original) * 100\n",
    "# percentage_removed, len(modified_df) / len(tmp_graph)\n",
    "\n",
    "# # Group by 'u' and 'i' and capture the first and last interactions\n",
    "# first_interactions = grouped.first().reset_index()\n",
    "# last_interactions = grouped.last().reset_index()\n",
    "\n",
    "# (len(first_interactions) + len(last_interactions)) / len(tmp_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on maximum mean shift strategy\n",
    "mean_shifts = compute_mean_shifts_with_metrics(tmp_graph, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'remove upto: {(1-upto):.2%}', 'length of mean shift is: ', len(mean_shifts), end=' ')\n",
    "\n",
    "threshold_index = int(len(mean_shifts) * (1-upto))\n",
    "\n",
    "print(f'{threshold_index=}')\n",
    "top_mean_shifts = mean_shifts[:threshold_index]\n",
    "print(len(top_mean_shifts), f'{len(top_mean_shifts) / len(mean_shifts):.2%}' )\n",
    "\n",
    "top_x_percent_timestamps = [ts for ts, _ in top_mean_shifts]\n",
    "print(len(top_x_percent_timestamps), f'{len(top_x_percent_timestamps) / len(train_graph_df[\"ts\"]):.2%}')\n",
    "\n",
    "# sampled_df = modified_df[~modified_df['ts'].isin(top_x_percent_timestamps)]\n",
    "sampled_df = tmp_graph[~tmp_graph['ts'].isin(top_x_percent_timestamps)]\n",
    "print(len(sampled_df['ts']), len(sampled_df['ts']) / len(tmp_graph['ts']))\n",
    "print('data sampling successful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_x_percent_timestamps) #  330\n",
    "len(tmp_graph['ts'])  # 110232\n",
    "\n",
    "len(set(tmp_graph['ts']).difference(set(top_x_percent_timestamps)))  # 106602\n",
    "len(set(tmp_graph['ts']).difference(set(top_x_percent_timestamps))) / len(tmp_graph['ts'])  # 106602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{scratch_location}/sparsified_data/{dataset_name}_{metric}_sparsified_{upto}.csv'\n",
    "# sampled_df.drop(['Unnamed: 0'], axis=1).to_csv(filename)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_data(tmp_graph, metric, upto, dataset_name, save=False):\n",
    "    # tmp_graph = tmp_graph.sort_values(by=['u', 'i', 'ts'])\n",
    "\n",
    "    # # Exclude the first and last rows based on 'u' and 'i'\n",
    "    # grouped = tmp_graph.groupby(['u', 'i'])\n",
    "    # modified_df = grouped.apply(lambda x: x.iloc[1:-1]).reset_index(drop=True)\n",
    "    \n",
    "    # based on maximum mean shift strategy\n",
    "    mean_shifts = compute_mean_shifts_with_metrics(tmp_graph, metric=metric)\n",
    "\n",
    "    print('back to sparsify_data file....')\n",
    "\n",
    "    threshold_index = int(len(mean_shifts) * (1-upto))\n",
    "    top_mean_shifts = mean_shifts[:threshold_index]\n",
    "\n",
    "    top_x_percent_timestamps = [ts for ts, _ in top_mean_shifts]\n",
    "\n",
    "    sampled_df = tmp_graph[~tmp_graph['ts'].isin(top_x_percent_timestamps)]\n",
    "    print('data sampling successful.')\n",
    "    \n",
    "    if save:\n",
    "        filename = f'{scratch_location}/sparsified_data/{dataset_name}_{metric}_sparsified_{upto}.csv'\n",
    "        sampled_df.drop(['Unnamed: 0'], axis=1).to_csv(filename)\n",
    "        print(filename, ' saved.')\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "In temporal_pagerank_heap_np dataset_name='reddit'\n",
      "mmap_file='/scratch/hmnshpl/reddit_ts_tpr_data.dat'\n",
      "\t inside tpr heap method\n",
      "Precomputed memory-mapped array loaded.      \n",
      "\t heapify successful\n",
      "\t out of loop.\n",
      "Calculated TPR      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating cumulative outgoing degree:  25%|██▍       | 115/469 [01:29<05:40,  1.04it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "list_of_upto = [0.9, 0.8, 0.7]\n",
    "metrics= ['ts_tpr_remove_combined_ter']  # ['cosine', 'euclidean', 'jaccard','kl_divergence', 'jensen_shannon_divergence', 'wasserstein']\n",
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "\n",
    "for upto in list_of_upto:\n",
    "    print(f'{upto}')\n",
    "    for metric in metrics:\n",
    "        print(f'\\t{metric}', end=' ')\n",
    "        # sampled_df = sparsify_data(tmp_graph, metric, upto, dataset_name, save=True)\n",
    "        ter_dict = calculate_combined_temporal_edgerank(tmp_graph, dataset_name=dataset_name)\n",
    "        \n",
    "        sorted_ter_dict = dict(sorted(ter_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        sorted_ter_dict = list(sorted_ter_dict.items())\n",
    "    \n",
    "        print('back to sparsify_data file....')\n",
    "\n",
    "        threshold_index = int(len(sorted_ter_dict) * (1-upto))\n",
    "\n",
    "        top_mean_shifts = sorted_ter_dict[:threshold_index]\n",
    "\n",
    "        top_x_percent_timestamps = [ts for ts, _ in top_mean_shifts]\n",
    "\n",
    "        sampled_df = tmp_graph[~tmp_graph['ts'].isin(top_x_percent_timestamps)]\n",
    "        sampled_df.drop(['Unnamed: 0'], axis=1).to_csv(filename)\n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# mmap_file = 'path/to/your/mmap_file'\n",
    "mmap_file=f'{scratch_location}/{dataset_name}_ts_tpr_data.dat'\n",
    "\n",
    "# Convert edges to a NumPy array\n",
    "E = tmp_graph[['u', 'i', 'ts']]\n",
    "E = np.array(E, dtype=[('u', int), ('v', int), ('t', float)])\n",
    "\n",
    "# Get the maximum node index to size the r and s arrays appropriately\n",
    "max_node = max(E['u'].max(), E['v'].max())\n",
    "\n",
    "if os.path.exists(mmap_file):\n",
    "    stats = os.stat(mmap_file)\n",
    "    mmap_size = stats.st_size\n",
    "    dtype = np.dtype([('t', float), ('r', float, max_node + 1)])\n",
    "    dtype_size = dtype.itemsize\n",
    "    data_size = mmap_size // dtype_size\n",
    "    print(f\"File size: {mmap_size/1024**3} Giga bytes\")\n",
    "    print(f\"Data type size: {dtype_size} bytes\")\n",
    "    print(f\"Number of items: {data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# sampled_df=pd.read_csv(filename)\n",
    "# # Remove columns with 'Unnamed:' in their name\n",
    "# sampled_df = sampled_df.loc[:, ~sampled_df.columns.str.contains('^Unnamed')]\n",
    "# sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shell_script(result_path, save_folder, email, dataset_name, model_name, patch_size,\n",
    "                        max_input_sequence_length, num_runs, gpu, sparsify, strategy, sampling_upto,\n",
    "                        num_cpus, num_gpus, gnode_name):\n",
    "    \n",
    "    # set Default variables\n",
    "    result_path = \"/home2/hmnshpl/projects/results\" if result_path is None else result_path\n",
    "    save_folder = \"DygLib\" if save_folder is None else save_folder\n",
    "    email = \"himanshu.pal@research.iiit.ac.in\" if email is None else email\n",
    "    dataset_name = \"wikipedia\" if dataset_name is None else dataset_name\n",
    "    model_name = \"TGN\" if model_name is None else model_name\n",
    "    patch_size = 2 if patch_size is None else patch_size\n",
    "    max_input_sequence_length = 64 if max_input_sequence_length is None else max_input_sequence_length\n",
    "    num_runs = 5 if num_runs is None else num_runs\n",
    "    gpu = 0 if gpu is None else gpu\n",
    "    sparsify = True if sparsify is None else sparsify\n",
    "    strategy = \"ts_tpr_remove_cosine\" if strategy is None else strategy\n",
    "    sampling_upto = 0.7 if sampling_upto is None else sampling_upto\n",
    "    num_cpus = 9 if num_cpus is None else num_cpus\n",
    "    num_gpus = 1 if num_gpus is None else num_gpus\n",
    "    if gnode_name is None or 'gnode' not in gnode_name:\n",
    "        raise ValueError(\"Please provide a valid gnode.\")\n",
    "\n",
    "    # Generate shell script content\n",
    "    script_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH -A research\n",
    "#SBATCH -n {num_cpus}\n",
    "#SBATCH --gres=gpu:{num_gpus}\n",
    "#SBATCH --mem-per-cpu=2G\n",
    "#SBATCH --output={result_path}/{save_folder}/Link_Prediciton_{strategy}_{sampling_upto}.txt\n",
    "#SBATCH --nodelist {gnode_name}\n",
    "#SBATCH --time=96:00:00\n",
    "#SBATCH --mail-user={email}\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "source ~/.bashrc\n",
    "\n",
    "conda activate tg\n",
    "\n",
    "python train_link_prediction.py --dataset_name {dataset_name} --model_name {model_name} --patch_size {patch_size} --max_input_sequence_length {max_input_sequence_length} --num_runs {num_runs} --gpu {gpu} --sparsify {sparsify} --strategy {strategy} --sampling_upto {sampling_upto}\n",
    "    \"\"\"\n",
    "\n",
    "    # Specify the output filename dynamically\n",
    "    output_filename = f\"../LP_{strategy}_{sampling_upto}.sh\"\n",
    "\n",
    "    # Write to file\n",
    "    with open(output_filename, \"w\") as file:\n",
    "        file.write(script_content)\n",
    "\n",
    "    print(f\"Shell script '{output_filename}' has been successfully generated.\")\n",
    "\n",
    "# Example usage\n",
    "# generate_shell_script(\"/home2/hmnshpl/projects/results\", \"DygLib\", \"himanshu.pal@research.iiit.ac.in\",\n",
    "#                     \"wikipedia\", \"TGN\", 2, 64, 5, 0, True, \"ts_tpr_remove_wasserstein\", 0.7, 9, 1, 'gnode085')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gnodes =  ['gnode074', 'gnode078', 'gnode067']\n",
    "strategies = ['ts_tpr_remove_chebyshev'] # ['ts_tpr_remove_MSS', 'ts_tpr_remove_mss_2', 'ts_tpr_remove_kl_divergence', 'ts_tpr_remove_jensen_shannon_divergence',\n",
    "            # 'ts_tpr_remove_cosine', 'ts_tpr_remove_euclidean', 'ts_tpr_remove_jaccard']\n",
    "samplings = [0.7, 0.8, 0.9]\n",
    "\n",
    "# Mapping of strategies to gnodes\n",
    "strategy_to_gnode = {\n",
    "    'ts_tpr_remove_wasserstein': 'gnode074',\n",
    "    'ts_tpr_remove_kl_divergence': 'gnode078',\n",
    "    'ts_tpr_remove_jensen_shannon_divergence': 'gnode067',\n",
    "    'ts_tpr_remove_MSS':'gnode074',\n",
    "    'ts_tpr_remove_mss_2':'gnode078',\n",
    "    'ts_tpr_remove_cosine': 'gnode067',\n",
    "    'ts_tpr_remove_euclidean': 'gnode074',\n",
    "    'ts_tpr_remove_jaccard': 'gnode078', \n",
    "    'ts_tpr_remove_chebyshev': 'gnode080', \n",
    "}\n",
    "\n",
    "for strategy in strategies:\n",
    "    for sampling in samplings:\n",
    "        gnode = strategy_to_gnode[strategy]\n",
    "        generate_shell_script(\"/home2/hmnshpl/projects/results\", \"DygLib\", \"himanshu.pal@research.iiit.ac.in\",\n",
    "                    \"wikipedia\", \"TGN\", 2, 64, 5, 0, True, strategy, sampling, 9, 1, gnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "# upto=0.7\n",
    "list_of_upto = [0.9, 0.8, 0.7]\n",
    "\n",
    "for upto in list_of_upto:\n",
    "    tmp_graph = tmp_graph.sort_values(by=['u', 'i', 'ts'])\n",
    "\n",
    "    # Exclude the first and last rows based on 'u' and 'i'\n",
    "    grouped = tmp_graph.groupby(['u', 'i'])\n",
    "    modified_df = grouped.apply(lambda x: x.iloc[1:-1]).reset_index(drop=True)\n",
    "\n",
    "    mean_shifts = mean_shift_removal2(tmp_graph)\n",
    "\n",
    "    print('back to sparsify_data file....')\n",
    "\n",
    "    threshold_index = int(len(mean_shifts) * (1-upto))\n",
    "    top_mean_shifts = mean_shifts[:threshold_index]\n",
    "\n",
    "    top_x_percent_timestamps = [ts for ts, _ in top_mean_shifts]\n",
    "\n",
    "    sampled_df = modified_df[~modified_df['ts'].isin(top_x_percent_timestamps)]\n",
    "\n",
    "    filename = f'{scratch_location}/sparsified_data/{dataset_name}_mss2_sparsified_{upto}.csv'\n",
    "    sampled_df.drop(['Unnamed: 0'], axis=1).to_csv(filename)\n",
    "    print(filename, ' saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "ter_dict = calculate_temporal_edge_rank(train_graph_df)\n",
    "len(ter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ter_dict = dict(sorted(ter_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "sorted_ter_dict = list(sorted_ter_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upto = 0.7\n",
    "print(f'remove upto: {(1-upto):.2%}', 'length of mean shift is: ', len(sorted_ter_dict), end=' ')\n",
    "\n",
    "threshold_index = int(len(sorted_ter_dict) * (1-upto))\n",
    "\n",
    "print(f'{threshold_index=}')\n",
    "top_mean_shifts = sorted_ter_dict[:threshold_index]\n",
    "print(len(top_mean_shifts), f'{len(top_mean_shifts) / len(sorted_ter_dict):.2%}' )\n",
    "\n",
    "top_x_percent_timestamps = [ts for ts, _ in top_mean_shifts]\n",
    "print(len(top_x_percent_timestamps), f'{len(top_x_percent_timestamps) / len(train_graph_df[\"ts\"]):.2%}')\n",
    "\n",
    "# sampled_df = modified_df[~modified_df['ts'].isin(top_x_percent_timestamps)]\n",
    "sampled_df = tmp_graph[~tmp_graph['ts'].isin(top_x_percent_timestamps)]\n",
    "print(len(sampled_df['ts']), len(sampled_df['ts']) / len(tmp_graph['ts']))\n",
    "print('data sampling successful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{scratch_location}/sparsified_data/{dataset_name}_TER_sparsified_{upto}.csv'\n",
    "sampled_df.drop(['Unnamed: 0'], axis=1).to_csv(filename)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined TER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "ter_dict = calculate_combined_temporal_edgerank(train_graph_df)\n",
    "len(ter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_graph_df['ts']), len(train_graph_df['ts'].unique()))\n",
    "\n",
    "sorted_ter_dict = dict(sorted(ter_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "sorted_ter_dict = list(sorted_ter_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upto = 0.9\n",
    "print(f'remove upto: {(1-upto):.2%}', 'length of mean shift is: ', len(sorted_ter_dict), end=' ')\n",
    "\n",
    "threshold_index = int(len(sorted_ter_dict) * (1-upto))\n",
    "\n",
    "print(f'{threshold_index=}')\n",
    "top_mean_shifts = sorted_ter_dict[:threshold_index]\n",
    "print(len(top_mean_shifts), f'{len(top_mean_shifts) / len(sorted_ter_dict):.2%}' )\n",
    "\n",
    "top_x_percent_timestamps = [ts for ts, _ in top_mean_shifts]\n",
    "print(len(top_x_percent_timestamps), f'{len(top_x_percent_timestamps) / len(train_graph_df[\"ts\"]):.2%}')\n",
    "\n",
    "# sampled_df = modified_df[~modified_df['ts'].isin(top_x_percent_timestamps)]\n",
    "sampled_df = tmp_graph[~tmp_graph['ts'].isin(top_x_percent_timestamps)]\n",
    "print(len(sampled_df['ts']), len(sampled_df['ts']) / len(tmp_graph['ts']))\n",
    "print('data sampling successful.')\n",
    "\n",
    "filename = f'{scratch_location}/sparsified_data/{dataset_name}_Combined_TER_sparsified_{upto}.csv'\n",
    "sampled_df.drop(['Unnamed: 0'], axis=1).to_csv(filename)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Incremental Sparsification code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Objective: To sparsify a network incrementally as in remove first top 10% timesteps then recalculate top 10% and so on... \"\"\"\n",
    "\n",
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "upto=0.7\n",
    "# metric='chebyshev'\n",
    "\n",
    "# based on maximum mean shift strategy\n",
    "# mean_shifts = compute_mean_shifts_with_metrics(tmp_graph, metric=metric)\n",
    "mean_shifts = mean_shift_removal(tmp_graph)\n",
    "\n",
    "print(f'remove upto: {(1-upto):.2%}', 'length of mean shift is: ', len(mean_shifts), end=' ')\n",
    "\n",
    "threshold_index = int(len(mean_shifts) * (1-upto))\n",
    "\n",
    "print(f'{threshold_index=}')\n",
    "top_mean_shifts = mean_shifts[:threshold_index]\n",
    "print(len(top_mean_shifts), f'{len(top_mean_shifts) / len(mean_shifts):.2%}' )\n",
    "\n",
    "top_x_percent_timestamps = [ts for ts, _ in top_mean_shifts]\n",
    "print(len(top_x_percent_timestamps), f'{len(top_x_percent_timestamps) / len(train_graph_df[\"ts\"]):.2%}')\n",
    "\n",
    "# sampled_df = modified_df[~modified_df['ts'].isin(top_x_percent_timestamps)]\n",
    "sampled_df = tmp_graph[~tmp_graph['ts'].isin(top_x_percent_timestamps)]\n",
    "print(len(sampled_df['ts']), len(sampled_df['ts']) / len(tmp_graph['ts']))\n",
    "print('data sampling successful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df_simple = sampled_df.copy(deep=True)   # non recursive removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "upto = 0.7\n",
    "\n",
    "# Calculate the percentage of data to be removed\n",
    "removal = (1 - upto) * 100\n",
    "\n",
    "# Loop through the specified percentage\n",
    "for i in tqdm(range(int(removal / 10)), desc='Processing'):\n",
    "    # Calculate mean shifts for the current graph state\n",
    "    mean_shifts = mean_shift_removal(tmp_graph)\n",
    "    \n",
    "    # Determine the index for the top 10% of mean shifts\n",
    "    threshold_index = int(len(mean_shifts) * 0.1)\n",
    "    \n",
    "    # Select the top 10% of mean shifts\n",
    "    top_mean_shifts = mean_shifts[:threshold_index]\n",
    "\n",
    "    # Extract the corresponding timestamps\n",
    "    top_x_percent_timestamps = [ts for ts, _ in top_mean_shifts]\n",
    "\n",
    "    # Remove rows corresponding to the top 10% of timestamps\n",
    "    tmp_graph = tmp_graph[~tmp_graph['ts'].isin(top_x_percent_timestamps)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tmp_graph['ts']), len(tmp_graph['ts']) / len(train_graph_df['ts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp_graph['ts'].unique()) / len(train_graph_df['ts']), len(sampled_df_simple['ts']) / len(train_graph_df['ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "full_data_percent = 1\n",
    "\n",
    "upto = 0.7\n",
    "total_removal_target = (full_data_percent - upto)  # Total of 10% removal\n",
    "removal_rate = 0.1\n",
    "remaining_steps = steps = int(total_removal_target // removal_rate)  # Number of steps\n",
    "\n",
    "print(f'To sparsify data {upto:.2%} at removal rate of {removal_rate:.2%}, we need {steps} steps.')\n",
    "\n",
    "for step in tqdm(range(steps), desc='Processing'):\n",
    "    current_removal = np.round(total_removal_target / (remaining_steps), decimals=3)\n",
    "    print(current_removal, remaining_steps)\n",
    "    full_data_percent -= current_removal\n",
    "    total_removal_target = (1 - upto/full_data_percent)\n",
    "    remaining_steps-=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "full_data_percent = 1\n",
    "\n",
    "upto = 0.7\n",
    "total_removal_target = (full_data_percent - upto)  # Total of 30% removal\n",
    "removal_rate = 0.1\n",
    "remaining_steps = steps = int(total_removal_target // removal_rate)  # Number of steps\n",
    "\n",
    "print(f'To sparsify data to {upto:.2%} at a removal rate of {removal_rate:.2%}, we need {steps} steps.')\n",
    "\n",
    "for step in tqdm(range(steps), desc='Processing'):\n",
    "    current_removal = removal_rate / (full_data_percent)  # Adjust the removal percentage\n",
    "    full_data_percent -= current_removal * full_data_percent  # Apply the removal\n",
    "\n",
    "    print(f\"Step {step + 1}: Removing {current_removal * 100:.3f}% of the original data\")\n",
    "    \n",
    "    remaining_steps -= 1\n",
    "full_data_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_graph = train_graph_df.copy(deep=True)\n",
    "upto = 0.7  # Target percentage of data to keep\n",
    "\n",
    "# Calculate the total percentage of data to be removed\n",
    "total_removal_target = 1 - upto\n",
    "remaining_data_fraction = 1.0  # Start with 100% of the data\n",
    "\n",
    "# Number of steps to remove 10% each time\n",
    "steps = int(total_removal_target / 0.1)\n",
    "\n",
    "print(f\"Target: Reduce to {upto:.2%} data in {steps} steps.\")\n",
    "\n",
    "for i in tqdm(range(steps), desc='Processing'):\n",
    "    # Adjust the removal target based on the remaining data\n",
    "    current_removal = 0.1 / remaining_data_fraction\n",
    "    \n",
    "    # Calculate mean shifts for the current graph state\n",
    "    mean_shifts = mean_shift_removal(tmp_graph)\n",
    "    \n",
    "    # Determine the index for the required top percentage of mean shifts\n",
    "    threshold_index = int(len(mean_shifts) * current_removal)\n",
    "    \n",
    "    # Select the top mean shifts\n",
    "    top_mean_shifts = mean_shifts[:threshold_index]\n",
    "\n",
    "    # Extract the corresponding timestamps\n",
    "    top_x_percent_timestamps = [ts for ts, _ in top_mean_shifts]\n",
    "\n",
    "    # Remove rows corresponding to the top timestamps\n",
    "    tmp_graph = tmp_graph[~tmp_graph['ts'].isin(top_x_percent_timestamps)]\n",
    "    \n",
    "    # Update the remaining data fraction\n",
    "    remaining_data_fraction -= current_removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp_graph['ts'].unique()) / len(train_graph_df['ts']) , len(sampled_df_simple['ts']) / len(train_graph_df['ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_sparse_ts = list(tmp_graph['ts'])\n",
    "non_rec_sparse_ts = list(sampled_df_simple['ts'])\n",
    "\n",
    "len(rec_sparse_ts), len(non_rec_sparse_ts), len(non_rec_sparse_ts) - len(rec_sparse_ts)\n",
    "\n",
    "# non-rec-sparsification has more ts than rec-sparsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(rec_sparse_ts)), len(set(non_rec_sparse_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(non_rec_sparse_ts).difference(set(rec_sparse_ts)))  # since there are 1268-159 new timestamps in new rec-sparsification mean we have change\n",
    "# in temporal rank of nodes in iteratively sparsified network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'{scratch_location}/sparsified_data/{dataset_name}_rec_mss_sparsified_{upto}.csv'\n",
    "sampled_df.drop(['Unnamed: 0'], axis=1).to_csv(filename)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
