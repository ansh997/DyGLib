INFO:root:********** Run 1 starts. **********
INFO:root:configuration is Namespace(dataset_name='wikipedia', batch_size=200, model_name='TGN', gpu=0, num_neighbors=20, sample_neighbor_strategy='recent', time_scaling_factor=1e-06, num_walk_heads=8, num_heads=2, num_layers=2, walk_length=1, time_gap=2000, time_feat_dim=100, position_feat_dim=172, edge_bank_memory_mode='unlimited_memory', time_window_mode='fixed_proportion', patch_size=2, channel_embedding_dim=50, max_input_sequence_length=64, learning_rate=0.0001, dropout=0.1, num_epochs=100, optimizer='Adam', weight_decay=0.0, patience=10, val_ratio=0.15, test_ratio=0.15, num_runs=5, test_interval_epochs=10, mrr=False, negative_sample_strategy='random', load_best_configs=False, sparsify=False, device='cuda:0', seed=0, save_model_name='TGN_seed0')
INFO:root:model -> Sequential(
  (0): MemoryModel(
    (time_encoder): TimeEncoder(
      (w): Linear(in_features=1, out_features=100, bias=True)
    )
    (message_aggregator): MessageAggregator()
    (memory_bank): MemoryBank(num_nodes=9228, memory_dim=172)
    (memory_updater): GRUMemoryUpdater(
      (memory_bank): MemoryBank(num_nodes=9228, memory_dim=172)
      (memory_updater): GRUCell(616, 172)
    )
    (embedding_module): GraphAttentionEmbedding(
      (time_encoder): TimeEncoder(
        (w): Linear(in_features=1, out_features=100, bias=True)
      )
      (temporal_conv_layers): ModuleList(
        (0-1): 2 x MultiHeadAttention(
          (query_projection): Linear(in_features=272, out_features=272, bias=False)
          (key_projection): Linear(in_features=444, out_features=272, bias=False)
          (value_projection): Linear(in_features=444, out_features=272, bias=False)
          (layer_norm): LayerNorm((272,), eps=1e-05, elementwise_affine=True)
          (residual_fc): Linear(in_features=272, out_features=272, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (merge_layers): ModuleList(
        (0-1): 2 x MergeLayer(
          (fc1): Linear(in_features=444, out_features=172, bias=True)
          (fc2): Linear(in_features=172, out_features=172, bias=True)
          (act): ReLU()
        )
      )
    )
  )
  (1): MergeLayer(
    (fc1): Linear(in_features=344, out_features=172, bias=True)
    (fc2): Linear(in_features=172, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:root:model name: TGN, #parameters: 5842340 B, 5705.41015625 KB, 5.571689605712891 MB.
The dataset has 157474 interactions, involving 9227 different nodes
The training dataset has 79202 interactions, involving 5904 different nodes
The validation dataset has 23621 interactions, involving 3256 different nodes
The test dataset has 23621 interactions, involving 3564 different nodes
The new node validation dataset has 11742 interactions, involving 2134 different nodes
The new node test dataset has 11765 interactions, involving 2482 different nodes
922 nodes were used for the inductive testing, i.e. are never seen during training
  0%|                                                                                           | 0/397 [00:00<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6941589117050171:   0%|                       | 0/397 [00:01<?, ?it/s]Epoch: 1, train for the 1-th batch, train loss: 0.6941589117050171:   0%|               | 1/397 [00:01<07:13,  1.09s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6921747922897339:   0%|               | 1/397 [00:01<07:13,  1.09s/it]Epoch: 1, train for the 2-th batch, train loss: 0.6921747922897339:   1%|               | 2/397 [00:01<04:18,  1.53it/s]Epoch: 1, train for the 3-th batch, train loss: 0.688115656375885:   1%|                | 2/397 [00:01<04:18,  1.53it/s]Epoch: 1, train for the 3-th batch, train loss: 0.688115656375885:   1%|                | 3/397 [00:01<03:18,  1.99it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6929342746734619:   1%|               | 3/397 [00:02<03:18,  1.99it/s]Epoch: 1, train for the 4-th batch, train loss: 0.6929342746734619:   1%|▏              | 4/397 [00:02<02:51,  2.30it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6915462613105774:   1%|▏              | 4/397 [00:02<02:51,  2.30it/s]Epoch: 1, train for the 5-th batch, train loss: 0.6915462613105774:   1%|▏              | 5/397 [00:02<02:40,  2.45it/s]Epoch: 1, train for the 6-th batch, train loss: 0.689437747001648:   1%|▏               | 5/397 [00:02<02:40,  2.45it/s]Epoch: 1, train for the 6-th batch, train loss: 0.689437747001648:   2%|▏               | 6/397 [00:02<02:31,  2.57it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6890503764152527:   2%|▏              | 6/397 [00:03<02:31,  2.57it/s]Epoch: 1, train for the 7-th batch, train loss: 0.6890503764152527:   2%|▎              | 7/397 [00:03<02:25,  2.67it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6844942569732666:   2%|▎              | 7/397 [00:03<02:25,  2.67it/s]Epoch: 1, train for the 8-th batch, train loss: 0.6844942569732666:   2%|▎              | 8/397 [00:03<02:19,  2.79it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6810328364372253:   2%|▎              | 8/397 [00:03<02:19,  2.79it/s]Epoch: 1, train for the 9-th batch, train loss: 0.6810328364372253:   2%|▎              | 9/397 [00:03<02:17,  2.82it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6813329458236694:   2%|▎             | 9/397 [00:04<02:17,  2.82it/s]Epoch: 1, train for the 10-th batch, train loss: 0.6813329458236694:   3%|▎            | 10/397 [00:04<02:15,  2.85it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6745384335517883:   3%|▎            | 10/397 [00:04<02:15,  2.85it/s]Epoch: 1, train for the 11-th batch, train loss: 0.6745384335517883:   3%|▎            | 11/397 [00:04<02:13,  2.89it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6733114719390869:   3%|▎            | 11/397 [00:04<02:13,  2.89it/s]Epoch: 1, train for the 12-th batch, train loss: 0.6733114719390869:   3%|▍            | 12/397 [00:04<02:10,  2.96it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6687282919883728:   3%|▍            | 12/397 [00:05<02:10,  2.96it/s]Epoch: 1, train for the 13-th batch, train loss: 0.6687282919883728:   3%|▍            | 13/397 [00:05<02:08,  3.00it/s]Epoch: 1, train for the 14-th batch, train loss: 0.661108672618866:   3%|▍             | 13/397 [00:05<02:08,  3.00it/s]Epoch: 1, train for the 14-th batch, train loss: 0.661108672618866:   4%|▍             | 14/397 [00:05<02:02,  3.12it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6636751890182495:   4%|▍            | 14/397 [00:05<02:02,  3.12it/s]Epoch: 1, train for the 15-th batch, train loss: 0.6636751890182495:   4%|▍            | 15/397 [00:05<02:02,  3.12it/s]Epoch: 1, train for the 16-th batch, train loss: 0.658141553401947:   4%|▌             | 15/397 [00:06<02:02,  3.12it/s]Epoch: 1, train for the 16-th batch, train loss: 0.658141553401947:   4%|▌             | 16/397 [00:06<02:02,  3.11it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6507250070571899:   4%|▌            | 16/397 [00:06<02:02,  3.11it/s]Epoch: 1, train for the 17-th batch, train loss: 0.6507250070571899:   4%|▌            | 17/397 [00:06<02:01,  3.14it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6377716064453125:   4%|▌            | 17/397 [00:06<02:01,  3.14it/s]Epoch: 1, train for the 18-th batch, train loss: 0.6377716064453125:   5%|▌            | 18/397 [00:06<01:59,  3.18it/s]Epoch: 1, train for the 19-th batch, train loss: 0.635077178478241:   5%|▋             | 18/397 [00:07<01:59,  3.18it/s]Epoch: 1, train for the 19-th batch, train loss: 0.635077178478241:   5%|▋             | 19/397 [00:07<01:59,  3.16it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6493929028511047:   5%|▌            | 19/397 [00:07<01:59,  3.16it/s]Epoch: 1, train for the 20-th batch, train loss: 0.6493929028511047:   5%|▋            | 20/397 [00:07<02:00,  3.14it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6345758438110352:   5%|▋            | 20/397 [00:07<02:00,  3.14it/s]Epoch: 1, train for the 21-th batch, train loss: 0.6345758438110352:   5%|▋            | 21/397 [00:07<01:58,  3.17it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6310336589813232:   5%|▋            | 21/397 [00:07<01:58,  3.17it/s]Epoch: 1, train for the 22-th batch, train loss: 0.6310336589813232:   6%|▋            | 22/397 [00:07<01:58,  3.15it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6336166262626648:   6%|▋            | 22/397 [00:08<01:58,  3.15it/s]Epoch: 1, train for the 23-th batch, train loss: 0.6336166262626648:   6%|▊            | 23/397 [00:08<01:57,  3.19it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6065235137939453:   6%|▊            | 23/397 [00:08<01:57,  3.19it/s]Epoch: 1, train for the 24-th batch, train loss: 0.6065235137939453:   6%|▊            | 24/397 [00:08<01:56,  3.20it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6084247827529907:   6%|▊            | 24/397 [00:08<01:56,  3.20it/s]Epoch: 1, train for the 25-th batch, train loss: 0.6084247827529907:   6%|▊            | 25/397 [00:08<01:55,  3.22it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5788081884384155:   6%|▊            | 25/397 [00:09<01:55,  3.22it/s]Epoch: 1, train for the 26-th batch, train loss: 0.5788081884384155:   7%|▊            | 26/397 [00:09<01:57,  3.16it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5793677568435669:   7%|▊            | 26/397 [00:09<01:57,  3.16it/s]Epoch: 1, train for the 27-th batch, train loss: 0.5793677568435669:   7%|▉            | 27/397 [00:09<01:57,  3.16it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5776275396347046:   7%|▉            | 27/397 [00:09<01:57,  3.16it/s]Epoch: 1, train for the 28-th batch, train loss: 0.5776275396347046:   7%|▉            | 28/397 [00:09<01:57,  3.15it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5544688701629639:   7%|▉            | 28/397 [00:10<01:57,  3.15it/s]Epoch: 1, train for the 29-th batch, train loss: 0.5544688701629639:   7%|▉            | 29/397 [00:10<01:56,  3.15it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5511083602905273:   7%|▉            | 29/397 [00:10<01:56,  3.15it/s]Epoch: 1, train for the 30-th batch, train loss: 0.5511083602905273:   8%|▉            | 30/397 [00:10<01:55,  3.16it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5491353273391724:   8%|▉            | 30/397 [00:10<01:55,  3.16it/s]Epoch: 1, train for the 31-th batch, train loss: 0.5491353273391724:   8%|█            | 31/397 [00:10<01:56,  3.15it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5450524091720581:   8%|█            | 31/397 [00:11<01:56,  3.15it/s]Epoch: 1, train for the 32-th batch, train loss: 0.5450524091720581:   8%|█            | 32/397 [00:11<01:55,  3.15it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5372466444969177:   8%|█            | 32/397 [00:11<01:55,  3.15it/s]Epoch: 1, train for the 33-th batch, train loss: 0.5372466444969177:   8%|█            | 33/397 [00:11<01:55,  3.15it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5052766799926758:   8%|█            | 33/397 [00:11<01:55,  3.15it/s]Epoch: 1, train for the 34-th batch, train loss: 0.5052766799926758:   9%|█            | 34/397 [00:11<01:54,  3.17it/s]Epoch: 1, train for the 35-th batch, train loss: 0.4922749996185303:   9%|█            | 34/397 [00:12<01:54,  3.17it/s]Epoch: 1, train for the 35-th batch, train loss: 0.4922749996185303:   9%|█▏           | 35/397 [00:12<01:53,  3.19it/s]Epoch: 1, train for the 36-th batch, train loss: 0.47291406989097595:   9%|█           | 35/397 [00:12<01:53,  3.19it/s]Epoch: 1, train for the 36-th batch, train loss: 0.47291406989097595:   9%|█           | 36/397 [00:12<01:51,  3.23it/s]Epoch: 1, train for the 37-th batch, train loss: 0.462095707654953:   9%|█▎            | 36/397 [00:12<01:51,  3.23it/s]Epoch: 1, train for the 37-th batch, train loss: 0.462095707654953:   9%|█▎            | 37/397 [00:12<01:50,  3.26it/s]Epoch: 1, train for the 38-th batch, train loss: 0.45779338479042053:   9%|█           | 37/397 [00:12<01:50,  3.26it/s]Epoch: 1, train for the 38-th batch, train loss: 0.45779338479042053:  10%|█▏          | 38/397 [00:12<01:50,  3.25it/s]Epoch: 1, train for the 39-th batch, train loss: 0.4492386281490326:  10%|█▏           | 38/397 [00:13<01:50,  3.25it/s]Epoch: 1, train for the 39-th batch, train loss: 0.4492386281490326:  10%|█▎           | 39/397 [00:13<01:50,  3.24it/s]Epoch: 1, train for the 40-th batch, train loss: 0.432309091091156:  10%|█▍            | 39/397 [00:13<01:50,  3.24it/s]Epoch: 1, train for the 40-th batch, train loss: 0.432309091091156:  10%|█▍            | 40/397 [00:13<01:48,  3.29it/s]Epoch: 1, train for the 41-th batch, train loss: 0.3908742368221283:  10%|█▎           | 40/397 [00:13<01:48,  3.29it/s]Epoch: 1, train for the 41-th batch, train loss: 0.3908742368221283:  10%|█▎           | 41/397 [00:13<01:48,  3.29it/s]Epoch: 1, train for the 42-th batch, train loss: 0.388305127620697:  10%|█▍            | 41/397 [00:14<01:48,  3.29it/s]Epoch: 1, train for the 42-th batch, train loss: 0.388305127620697:  11%|█▍            | 42/397 [00:14<01:48,  3.27it/s]Epoch: 1, train for the 43-th batch, train loss: 0.3784956932067871:  11%|█▍           | 42/397 [00:14<01:48,  3.27it/s]Epoch: 1, train for the 43-th batch, train loss: 0.3784956932067871:  11%|█▍           | 43/397 [00:14<01:47,  3.29it/s]Epoch: 1, train for the 44-th batch, train loss: 0.34065285325050354:  11%|█▎          | 43/397 [00:14<01:47,  3.29it/s]Epoch: 1, train for the 44-th batch, train loss: 0.34065285325050354:  11%|█▎          | 44/397 [00:14<01:45,  3.35it/s]Epoch: 1, train for the 45-th batch, train loss: 0.3652169704437256:  11%|█▍           | 44/397 [00:15<01:45,  3.35it/s]Epoch: 1, train for the 45-th batch, train loss: 0.3652169704437256:  11%|█▍           | 45/397 [00:15<01:44,  3.35it/s]Epoch: 1, train for the 46-th batch, train loss: 0.35016950964927673:  11%|█▎          | 45/397 [00:15<01:44,  3.35it/s]Epoch: 1, train for the 46-th batch, train loss: 0.35016950964927673:  12%|█▍          | 46/397 [00:15<01:46,  3.28it/s]Epoch: 1, train for the 47-th batch, train loss: 0.3968643844127655:  12%|█▌           | 46/397 [00:15<01:46,  3.28it/s]Epoch: 1, train for the 47-th batch, train loss: 0.3968643844127655:  12%|█▌           | 47/397 [00:15<01:48,  3.23it/s]Epoch: 1, train for the 48-th batch, train loss: 0.36588579416275024:  12%|█▍          | 47/397 [00:16<01:48,  3.23it/s]Epoch: 1, train for the 48-th batch, train loss: 0.36588579416275024:  12%|█▍          | 48/397 [00:16<01:51,  3.14it/s]Epoch: 1, train for the 49-th batch, train loss: 0.2859449088573456:  12%|█▌           | 48/397 [00:16<01:51,  3.14it/s]Epoch: 1, train for the 49-th batch, train loss: 0.2859449088573456:  12%|█▌           | 49/397 [00:16<01:48,  3.21it/s]Epoch: 1, train for the 50-th batch, train loss: 0.35358068346977234:  12%|█▍          | 49/397 [00:16<01:48,  3.21it/s]Epoch: 1, train for the 50-th batch, train loss: 0.35358068346977234:  13%|█▌          | 50/397 [00:16<01:47,  3.23it/s]Epoch: 1, train for the 51-th batch, train loss: 0.3325936198234558:  13%|█▋           | 50/397 [00:16<01:47,  3.23it/s]Epoch: 1, train for the 51-th batch, train loss: 0.3325936198234558:  13%|█▋           | 51/397 [00:16<01:46,  3.24it/s]Epoch: 1, train for the 52-th batch, train loss: 0.30467110872268677:  13%|█▌          | 51/397 [00:17<01:46,  3.24it/s]Epoch: 1, train for the 52-th batch, train loss: 0.30467110872268677:  13%|█▌          | 52/397 [00:17<01:44,  3.30it/s]Epoch: 1, train for the 53-th batch, train loss: 0.33803388476371765:  13%|█▌          | 52/397 [00:17<01:44,  3.30it/s]Epoch: 1, train for the 53-th batch, train loss: 0.33803388476371765:  13%|█▌          | 53/397 [00:17<01:45,  3.27it/s]Epoch: 1, train for the 54-th batch, train loss: 0.3160938024520874:  13%|█▋           | 53/397 [00:17<01:45,  3.27it/s]Epoch: 1, train for the 54-th batch, train loss: 0.3160938024520874:  14%|█▊           | 54/397 [00:17<01:46,  3.22it/s]Epoch: 1, train for the 55-th batch, train loss: 0.3515826463699341:  14%|█▊           | 54/397 [00:18<01:46,  3.22it/s]Epoch: 1, train for the 55-th batch, train loss: 0.3515826463699341:  14%|█▊           | 55/397 [00:18<01:46,  3.22it/s]Epoch: 1, train for the 56-th batch, train loss: 0.3323504626750946:  14%|█▊           | 55/397 [00:18<01:46,  3.22it/s]Epoch: 1, train for the 56-th batch, train loss: 0.3323504626750946:  14%|█▊           | 56/397 [00:18<01:46,  3.20it/s]Epoch: 1, train for the 57-th batch, train loss: 0.2762889266014099:  14%|█▊           | 56/397 [00:18<01:46,  3.20it/s]Epoch: 1, train for the 57-th batch, train loss: 0.2762889266014099:  14%|█▊           | 57/397 [00:18<01:45,  3.23it/s]Epoch: 1, train for the 58-th batch, train loss: 0.3530665934085846:  14%|█▊           | 57/397 [00:19<01:45,  3.23it/s]Epoch: 1, train for the 58-th batch, train loss: 0.3530665934085846:  15%|█▉           | 58/397 [00:19<01:45,  3.20it/s]Epoch: 1, train for the 59-th batch, train loss: 0.342415988445282:  15%|██            | 58/397 [00:19<01:45,  3.20it/s]Epoch: 1, train for the 59-th batch, train loss: 0.342415988445282:  15%|██            | 59/397 [00:19<01:45,  3.20it/s]Epoch: 1, train for the 60-th batch, train loss: 0.3334811329841614:  15%|█▉           | 59/397 [00:19<01:45,  3.20it/s]Epoch: 1, train for the 60-th batch, train loss: 0.3334811329841614:  15%|█▉           | 60/397 [00:19<01:45,  3.20it/s]Epoch: 1, train for the 61-th batch, train loss: 0.32547807693481445:  15%|█▊          | 60/397 [00:20<01:45,  3.20it/s]Epoch: 1, train for the 61-th batch, train loss: 0.32547807693481445:  15%|█▊          | 61/397 [00:20<01:43,  3.25it/s]Epoch: 1, train for the 62-th batch, train loss: 0.28695231676101685:  15%|█▊          | 61/397 [00:20<01:43,  3.25it/s]Epoch: 1, train for the 62-th batch, train loss: 0.28695231676101685:  16%|█▊          | 62/397 [00:20<01:41,  3.30it/s]Epoch: 1, train for the 63-th batch, train loss: 0.3374294638633728:  16%|██           | 62/397 [00:20<01:41,  3.30it/s]Epoch: 1, train for the 63-th batch, train loss: 0.3374294638633728:  16%|██           | 63/397 [00:20<01:42,  3.27it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3313058018684387:  16%|██           | 63/397 [00:20<01:42,  3.27it/s]Epoch: 1, train for the 64-th batch, train loss: 0.3313058018684387:  16%|██           | 64/397 [00:20<01:41,  3.27it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4372228980064392:  16%|██           | 64/397 [00:21<01:41,  3.27it/s]Epoch: 1, train for the 65-th batch, train loss: 0.4372228980064392:  16%|██▏          | 65/397 [00:21<01:41,  3.26it/s]Epoch: 1, train for the 66-th batch, train loss: 0.39093124866485596:  16%|█▉          | 65/397 [00:21<01:41,  3.26it/s]Epoch: 1, train for the 66-th batch, train loss: 0.39093124866485596:  17%|█▉          | 66/397 [00:21<01:40,  3.30it/s]Epoch: 1, train for the 67-th batch, train loss: 0.38784563541412354:  17%|█▉          | 66/397 [00:21<01:40,  3.30it/s]Epoch: 1, train for the 67-th batch, train loss: 0.38784563541412354:  17%|██          | 67/397 [00:21<01:40,  3.28it/s]Epoch: 1, train for the 68-th batch, train loss: 0.34325534105300903:  17%|██          | 67/397 [00:22<01:40,  3.28it/s]Epoch: 1, train for the 68-th batch, train loss: 0.34325534105300903:  17%|██          | 68/397 [00:22<01:40,  3.27it/s]Epoch: 1, train for the 69-th batch, train loss: 0.36529722809791565:  17%|██          | 68/397 [00:22<01:40,  3.27it/s]Epoch: 1, train for the 69-th batch, train loss: 0.36529722809791565:  17%|██          | 69/397 [00:22<01:40,  3.25it/s]Epoch: 1, train for the 70-th batch, train loss: 0.3590104579925537:  17%|██▎          | 69/397 [00:22<01:40,  3.25it/s]Epoch: 1, train for the 70-th batch, train loss: 0.3590104579925537:  18%|██▎          | 70/397 [00:22<01:40,  3.26it/s]Epoch: 1, train for the 71-th batch, train loss: 0.33986157178878784:  18%|██          | 70/397 [00:23<01:40,  3.26it/s]Epoch: 1, train for the 71-th batch, train loss: 0.33986157178878784:  18%|██▏         | 71/397 [00:23<01:41,  3.21it/s]Epoch: 1, train for the 72-th batch, train loss: 0.2949902415275574:  18%|██▎          | 71/397 [00:23<01:41,  3.21it/s]Epoch: 1, train for the 72-th batch, train loss: 0.2949902415275574:  18%|██▎          | 72/397 [00:23<01:41,  3.21it/s]Epoch: 1, train for the 73-th batch, train loss: 0.2313058078289032:  18%|██▎          | 72/397 [00:23<01:41,  3.21it/s]Epoch: 1, train for the 73-th batch, train loss: 0.2313058078289032:  18%|██▍          | 73/397 [00:23<01:40,  3.21it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2452578991651535:  18%|██▍          | 73/397 [00:24<01:40,  3.21it/s]Epoch: 1, train for the 74-th batch, train loss: 0.2452578991651535:  19%|██▍          | 74/397 [00:24<01:40,  3.22it/s]Epoch: 1, train for the 75-th batch, train loss: 0.27990055084228516:  19%|██▏         | 74/397 [00:24<01:40,  3.22it/s]Epoch: 1, train for the 75-th batch, train loss: 0.27990055084228516:  19%|██▎         | 75/397 [00:24<01:39,  3.24it/s]Epoch: 1, train for the 76-th batch, train loss: 0.3407512307167053:  19%|██▍          | 75/397 [00:24<01:39,  3.24it/s]Epoch: 1, train for the 76-th batch, train loss: 0.3407512307167053:  19%|██▍          | 76/397 [00:24<01:38,  3.27it/s]Epoch: 1, train for the 77-th batch, train loss: 0.3370192348957062:  19%|██▍          | 76/397 [00:24<01:38,  3.27it/s]Epoch: 1, train for the 77-th batch, train loss: 0.3370192348957062:  19%|██▌          | 77/397 [00:24<01:37,  3.28it/s]Epoch: 1, train for the 78-th batch, train loss: 0.36773911118507385:  19%|██▎         | 77/397 [00:25<01:37,  3.28it/s]Epoch: 1, train for the 78-th batch, train loss: 0.36773911118507385:  20%|██▎         | 78/397 [00:25<01:36,  3.29it/s]Epoch: 1, train for the 79-th batch, train loss: 0.3109199106693268:  20%|██▌          | 78/397 [00:25<01:36,  3.29it/s]Epoch: 1, train for the 79-th batch, train loss: 0.3109199106693268:  20%|██▌          | 79/397 [00:25<01:36,  3.30it/s]Epoch: 1, train for the 80-th batch, train loss: 0.2697128653526306:  20%|██▌          | 79/397 [00:25<01:36,  3.30it/s]Epoch: 1, train for the 80-th batch, train loss: 0.2697128653526306:  20%|██▌          | 80/397 [00:25<01:34,  3.34it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3112790584564209:  20%|██▌          | 80/397 [00:26<01:34,  3.34it/s]Epoch: 1, train for the 81-th batch, train loss: 0.3112790584564209:  20%|██▋          | 81/397 [00:26<01:35,  3.31it/s]Epoch: 1, train for the 82-th batch, train loss: 0.41074228286743164:  20%|██▍         | 81/397 [00:26<01:35,  3.31it/s]Epoch: 1, train for the 82-th batch, train loss: 0.41074228286743164:  21%|██▍         | 82/397 [00:26<01:34,  3.34it/s]Epoch: 1, train for the 83-th batch, train loss: 0.2937077283859253:  21%|██▋          | 82/397 [00:26<01:34,  3.34it/s]Epoch: 1, train for the 83-th batch, train loss: 0.2937077283859253:  21%|██▋          | 83/397 [00:26<01:32,  3.39it/s]Epoch: 1, train for the 84-th batch, train loss: 0.29005321860313416:  21%|██▌         | 83/397 [00:27<01:32,  3.39it/s]Epoch: 1, train for the 84-th batch, train loss: 0.29005321860313416:  21%|██▌         | 84/397 [00:27<01:32,  3.39it/s]Epoch: 1, train for the 85-th batch, train loss: 0.26185446977615356:  21%|██▌         | 84/397 [00:27<01:32,  3.39it/s]Epoch: 1, train for the 85-th batch, train loss: 0.26185446977615356:  21%|██▌         | 85/397 [00:27<01:33,  3.35it/s]Epoch: 1, train for the 86-th batch, train loss: 0.362322598695755:  21%|██▉           | 85/397 [00:27<01:33,  3.35it/s]Epoch: 1, train for the 86-th batch, train loss: 0.362322598695755:  22%|███           | 86/397 [00:27<01:32,  3.38it/s]Epoch: 1, train for the 87-th batch, train loss: 0.3267655074596405:  22%|██▊          | 86/397 [00:27<01:32,  3.38it/s]Epoch: 1, train for the 87-th batch, train loss: 0.3267655074596405:  22%|██▊          | 87/397 [00:27<01:31,  3.38it/s]Epoch: 1, train for the 88-th batch, train loss: 0.30705922842025757:  22%|██▋         | 87/397 [00:28<01:31,  3.38it/s]Epoch: 1, train for the 88-th batch, train loss: 0.30705922842025757:  22%|██▋         | 88/397 [00:28<01:31,  3.39it/s]Epoch: 1, train for the 89-th batch, train loss: 0.275770366191864:  22%|███           | 88/397 [00:28<01:31,  3.39it/s]Epoch: 1, train for the 89-th batch, train loss: 0.275770366191864:  22%|███▏          | 89/397 [00:28<01:32,  3.34it/s]Epoch: 1, train for the 90-th batch, train loss: 0.38790228962898254:  22%|██▋         | 89/397 [00:28<01:32,  3.34it/s]Epoch: 1, train for the 90-th batch, train loss: 0.38790228962898254:  23%|██▋         | 90/397 [00:28<01:34,  3.25it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3429591953754425:  23%|██▉          | 90/397 [00:29<01:34,  3.25it/s]Epoch: 1, train for the 91-th batch, train loss: 0.3429591953754425:  23%|██▉          | 91/397 [00:29<01:34,  3.25it/s]Epoch: 1, train for the 92-th batch, train loss: 0.360577255487442:  23%|███▏          | 91/397 [00:29<01:34,  3.25it/s]Epoch: 1, train for the 92-th batch, train loss: 0.360577255487442:  23%|███▏          | 92/397 [00:29<01:33,  3.27it/s]Epoch: 1, train for the 93-th batch, train loss: 0.28844940662384033:  23%|██▊         | 92/397 [00:29<01:33,  3.27it/s]Epoch: 1, train for the 93-th batch, train loss: 0.28844940662384033:  23%|██▊         | 93/397 [00:29<01:32,  3.27it/s]Epoch: 1, train for the 94-th batch, train loss: 0.3062058091163635:  23%|███          | 93/397 [00:30<01:32,  3.27it/s]Epoch: 1, train for the 94-th batch, train loss: 0.3062058091163635:  24%|███          | 94/397 [00:30<01:32,  3.29it/s]Epoch: 1, train for the 95-th batch, train loss: 0.30411794781684875:  24%|██▊         | 94/397 [00:30<01:32,  3.29it/s]Epoch: 1, train for the 95-th batch, train loss: 0.30411794781684875:  24%|██▊         | 95/397 [00:30<01:31,  3.31it/s]Epoch: 1, train for the 96-th batch, train loss: 0.21694770455360413:  24%|██▊         | 95/397 [00:30<01:31,  3.31it/s]Epoch: 1, train for the 96-th batch, train loss: 0.21694770455360413:  24%|██▉         | 96/397 [00:30<01:30,  3.33it/s]Epoch: 1, train for the 97-th batch, train loss: 0.27388593554496765:  24%|██▉         | 96/397 [00:30<01:30,  3.33it/s]Epoch: 1, train for the 97-th batch, train loss: 0.27388593554496765:  24%|██▉         | 97/397 [00:30<01:29,  3.35it/s]Epoch: 1, train for the 98-th batch, train loss: 0.2171640694141388:  24%|███▏         | 97/397 [00:31<01:29,  3.35it/s]Epoch: 1, train for the 98-th batch, train loss: 0.2171640694141388:  25%|███▏         | 98/397 [00:31<01:26,  3.44it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3378455936908722:  25%|███▏         | 98/397 [00:31<01:26,  3.44it/s]Epoch: 1, train for the 99-th batch, train loss: 0.3378455936908722:  25%|███▏         | 99/397 [00:31<01:25,  3.48it/s]Epoch: 1, train for the 100-th batch, train loss: 0.3184798061847687:  25%|██▉         | 99/397 [00:31<01:25,  3.48it/s]Epoch: 1, train for the 100-th batch, train loss: 0.3184798061847687:  25%|██▊        | 100/397 [00:31<01:25,  3.46it/s]Epoch: 1, train for the 101-th batch, train loss: 0.254533052444458:  25%|███         | 100/397 [00:32<01:25,  3.46it/s]Epoch: 1, train for the 101-th batch, train loss: 0.254533052444458:  25%|███         | 101/397 [00:32<01:24,  3.49it/s]Epoch: 1, train for the 102-th batch, train loss: 0.322449266910553:  25%|███         | 101/397 [00:32<01:24,  3.49it/s]Epoch: 1, train for the 102-th batch, train loss: 0.322449266910553:  26%|███         | 102/397 [00:32<01:23,  3.54it/s]Epoch: 1, train for the 103-th batch, train loss: 0.2741766571998596:  26%|██▊        | 102/397 [00:32<01:23,  3.54it/s]Epoch: 1, train for the 103-th batch, train loss: 0.2741766571998596:  26%|██▊        | 103/397 [00:32<01:23,  3.51it/s]Epoch: 1, train for the 104-th batch, train loss: 0.2941271960735321:  26%|██▊        | 103/397 [00:32<01:23,  3.51it/s]Epoch: 1, train for the 104-th batch, train loss: 0.2941271960735321:  26%|██▉        | 104/397 [00:32<01:27,  3.34it/s]Epoch: 1, train for the 105-th batch, train loss: 0.27877628803253174:  26%|██▌       | 104/397 [00:33<01:27,  3.34it/s]Epoch: 1, train for the 105-th batch, train loss: 0.27877628803253174:  26%|██▋       | 105/397 [00:33<01:27,  3.33it/s]Epoch: 1, train for the 106-th batch, train loss: 0.26063767075538635:  26%|██▋       | 105/397 [00:33<01:27,  3.33it/s]Epoch: 1, train for the 106-th batch, train loss: 0.26063767075538635:  27%|██▋       | 106/397 [00:33<01:26,  3.35it/s]Epoch: 1, train for the 107-th batch, train loss: 0.3342001140117645:  27%|██▉        | 106/397 [00:33<01:26,  3.35it/s]Epoch: 1, train for the 107-th batch, train loss: 0.3342001140117645:  27%|██▉        | 107/397 [00:33<01:27,  3.32it/s]Epoch: 1, train for the 108-th batch, train loss: 0.31029558181762695:  27%|██▋       | 107/397 [00:34<01:27,  3.32it/s]Epoch: 1, train for the 108-th batch, train loss: 0.31029558181762695:  27%|██▋       | 108/397 [00:34<01:27,  3.32it/s]Epoch: 1, train for the 109-th batch, train loss: 0.35760626196861267:  27%|██▋       | 108/397 [00:34<01:27,  3.32it/s]Epoch: 1, train for the 109-th batch, train loss: 0.35760626196861267:  27%|██▋       | 109/397 [00:34<01:27,  3.29it/s]Epoch: 1, train for the 110-th batch, train loss: 0.2591150104999542:  27%|███        | 109/397 [00:34<01:27,  3.29it/s]Epoch: 1, train for the 110-th batch, train loss: 0.2591150104999542:  28%|███        | 110/397 [00:34<01:26,  3.32it/s]Epoch: 1, train for the 111-th batch, train loss: 0.3015884459018707:  28%|███        | 110/397 [00:35<01:26,  3.32it/s]Epoch: 1, train for the 111-th batch, train loss: 0.3015884459018707:  28%|███        | 111/397 [00:35<01:24,  3.37it/s]Epoch: 1, train for the 112-th batch, train loss: 0.3610772490501404:  28%|███        | 111/397 [00:35<01:24,  3.37it/s]Epoch: 1, train for the 112-th batch, train loss: 0.3610772490501404:  28%|███        | 112/397 [00:35<01:24,  3.38it/s]Epoch: 1, train for the 113-th batch, train loss: 0.34092292189598083:  28%|██▊       | 112/397 [00:35<01:24,  3.38it/s]Epoch: 1, train for the 113-th batch, train loss: 0.34092292189598083:  28%|██▊       | 113/397 [00:35<01:23,  3.40it/s]Epoch: 1, train for the 114-th batch, train loss: 0.34901922941207886:  28%|██▊       | 113/397 [00:35<01:23,  3.40it/s]Epoch: 1, train for the 114-th batch, train loss: 0.34901922941207886:  29%|██▊       | 114/397 [00:35<01:23,  3.39it/s]Epoch: 1, train for the 115-th batch, train loss: 0.3387511074542999:  29%|███▏       | 114/397 [00:36<01:23,  3.39it/s]Epoch: 1, train for the 115-th batch, train loss: 0.3387511074542999:  29%|███▏       | 115/397 [00:36<01:24,  3.35it/s]Epoch: 1, train for the 116-th batch, train loss: 0.3007727563381195:  29%|███▏       | 115/397 [00:36<01:24,  3.35it/s]Epoch: 1, train for the 116-th batch, train loss: 0.3007727563381195:  29%|███▏       | 116/397 [00:36<01:23,  3.37it/s]Epoch: 1, train for the 117-th batch, train loss: 0.31837955117225647:  29%|██▉       | 116/397 [00:36<01:23,  3.37it/s]Epoch: 1, train for the 117-th batch, train loss: 0.31837955117225647:  29%|██▉       | 117/397 [00:36<01:22,  3.38it/s]Epoch: 1, train for the 118-th batch, train loss: 0.33796852827072144:  29%|██▉       | 117/397 [00:37<01:22,  3.38it/s]Epoch: 1, train for the 118-th batch, train loss: 0.33796852827072144:  30%|██▉       | 118/397 [00:37<01:23,  3.32it/s]Epoch: 1, train for the 119-th batch, train loss: 0.3055402934551239:  30%|███▎       | 118/397 [00:37<01:23,  3.32it/s]Epoch: 1, train for the 119-th batch, train loss: 0.3055402934551239:  30%|███▎       | 119/397 [00:37<01:25,  3.25it/s]Epoch: 1, train for the 120-th batch, train loss: 0.2460719645023346:  30%|███▎       | 119/397 [00:37<01:25,  3.25it/s]Epoch: 1, train for the 120-th batch, train loss: 0.2460719645023346:  30%|███▎       | 120/397 [00:37<01:24,  3.30it/s]Epoch: 1, train for the 121-th batch, train loss: 0.24968364834785461:  30%|███       | 120/397 [00:38<01:24,  3.30it/s]Epoch: 1, train for the 121-th batch, train loss: 0.24968364834785461:  30%|███       | 121/397 [00:38<01:22,  3.36it/s]Epoch: 1, train for the 122-th batch, train loss: 0.3831135928630829:  30%|███▎       | 121/397 [00:38<01:22,  3.36it/s]Epoch: 1, train for the 122-th batch, train loss: 0.3831135928630829:  31%|███▍       | 122/397 [00:38<01:20,  3.40it/s]Epoch: 1, train for the 123-th batch, train loss: 0.2998315691947937:  31%|███▍       | 122/397 [00:38<01:20,  3.40it/s]Epoch: 1, train for the 123-th batch, train loss: 0.2998315691947937:  31%|███▍       | 123/397 [00:38<01:19,  3.44it/s]Epoch: 1, train for the 124-th batch, train loss: 0.32632675766944885:  31%|███       | 123/397 [00:38<01:19,  3.44it/s]Epoch: 1, train for the 124-th batch, train loss: 0.32632675766944885:  31%|███       | 124/397 [00:38<01:20,  3.41it/s]Epoch: 1, train for the 125-th batch, train loss: 0.29115936160087585:  31%|███       | 124/397 [00:39<01:20,  3.41it/s]Epoch: 1, train for the 125-th batch, train loss: 0.29115936160087585:  31%|███▏      | 125/397 [00:39<01:19,  3.42it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3166676163673401:  31%|███▍       | 125/397 [00:39<01:19,  3.42it/s]Epoch: 1, train for the 126-th batch, train loss: 0.3166676163673401:  32%|███▍       | 126/397 [00:39<01:18,  3.47it/s]Epoch: 1, train for the 127-th batch, train loss: 0.3663281798362732:  32%|███▍       | 126/397 [00:39<01:18,  3.47it/s]Epoch: 1, train for the 127-th batch, train loss: 0.3663281798362732:  32%|███▌       | 127/397 [00:39<01:18,  3.45it/s]Epoch: 1, train for the 128-th batch, train loss: 0.3455827236175537:  32%|███▌       | 127/397 [00:40<01:18,  3.45it/s]Epoch: 1, train for the 128-th batch, train loss: 0.3455827236175537:  32%|███▌       | 128/397 [00:40<01:17,  3.48it/s]Epoch: 1, train for the 129-th batch, train loss: 0.3135545253753662:  32%|███▌       | 128/397 [00:40<01:17,  3.48it/s]Epoch: 1, train for the 129-th batch, train loss: 0.3135545253753662:  32%|███▌       | 129/397 [00:40<01:16,  3.50it/s]Epoch: 1, train for the 130-th batch, train loss: 0.2826571762561798:  32%|███▌       | 129/397 [00:40<01:16,  3.50it/s]Epoch: 1, train for the 130-th batch, train loss: 0.2826571762561798:  33%|███▌       | 130/397 [00:40<01:16,  3.49it/s]Epoch: 1, train for the 131-th batch, train loss: 0.3064406216144562:  33%|███▌       | 130/397 [00:40<01:16,  3.49it/s]Epoch: 1, train for the 131-th batch, train loss: 0.3064406216144562:  33%|███▋       | 131/397 [00:40<01:16,  3.48it/s]Epoch: 1, train for the 132-th batch, train loss: 0.32017040252685547:  33%|███▎      | 131/397 [00:41<01:16,  3.48it/s]Epoch: 1, train for the 132-th batch, train loss: 0.32017040252685547:  33%|███▎      | 132/397 [00:41<01:15,  3.50it/s]Epoch: 1, train for the 133-th batch, train loss: 0.39103835821151733:  33%|███▎      | 132/397 [00:41<01:15,  3.50it/s]Epoch: 1, train for the 133-th batch, train loss: 0.39103835821151733:  34%|███▎      | 133/397 [00:41<01:15,  3.47it/s]Epoch: 1, train for the 134-th batch, train loss: 0.30539894104003906:  34%|███▎      | 133/397 [00:41<01:15,  3.47it/s]Epoch: 1, train for the 134-th batch, train loss: 0.30539894104003906:  34%|███▍      | 134/397 [00:41<01:15,  3.50it/s]Epoch: 1, train for the 135-th batch, train loss: 0.2770203649997711:  34%|███▋       | 134/397 [00:42<01:15,  3.50it/s]Epoch: 1, train for the 135-th batch, train loss: 0.2770203649997711:  34%|███▋       | 135/397 [00:42<01:15,  3.49it/s]Epoch: 1, train for the 136-th batch, train loss: 0.35635626316070557:  34%|███▍      | 135/397 [00:42<01:15,  3.49it/s]Epoch: 1, train for the 136-th batch, train loss: 0.35635626316070557:  34%|███▍      | 136/397 [00:42<01:14,  3.50it/s]Epoch: 1, train for the 137-th batch, train loss: 0.280048668384552:  34%|████        | 136/397 [00:42<01:14,  3.50it/s]Epoch: 1, train for the 137-th batch, train loss: 0.280048668384552:  35%|████▏       | 137/397 [00:42<01:14,  3.51it/s]Epoch: 1, train for the 138-th batch, train loss: 0.2685241997241974:  35%|███▊       | 137/397 [00:42<01:14,  3.51it/s]Epoch: 1, train for the 138-th batch, train loss: 0.2685241997241974:  35%|███▊       | 138/397 [00:42<01:14,  3.50it/s]Epoch: 1, train for the 139-th batch, train loss: 0.21799275279045105:  35%|███▍      | 138/397 [00:43<01:14,  3.50it/s]Epoch: 1, train for the 139-th batch, train loss: 0.21799275279045105:  35%|███▌      | 139/397 [00:43<01:13,  3.51it/s]Epoch: 1, train for the 140-th batch, train loss: 0.26375699043273926:  35%|███▌      | 139/397 [00:43<01:13,  3.51it/s]Epoch: 1, train for the 140-th batch, train loss: 0.26375699043273926:  35%|███▌      | 140/397 [00:43<01:12,  3.56it/s]Epoch: 1, train for the 141-th batch, train loss: 0.28797295689582825:  35%|███▌      | 140/397 [00:43<01:12,  3.56it/s]Epoch: 1, train for the 141-th batch, train loss: 0.28797295689582825:  36%|███▌      | 141/397 [00:43<01:11,  3.58it/s]Epoch: 1, train for the 142-th batch, train loss: 0.2508165240287781:  36%|███▉       | 141/397 [00:44<01:11,  3.58it/s]Epoch: 1, train for the 142-th batch, train loss: 0.2508165240287781:  36%|███▉       | 142/397 [00:44<01:11,  3.58it/s]Epoch: 1, train for the 143-th batch, train loss: 0.29414957761764526:  36%|███▌      | 142/397 [00:44<01:11,  3.58it/s]Epoch: 1, train for the 143-th batch, train loss: 0.29414957761764526:  36%|███▌      | 143/397 [00:44<01:11,  3.54it/s]Epoch: 1, train for the 144-th batch, train loss: 0.2596157491207123:  36%|███▉       | 143/397 [00:44<01:11,  3.54it/s]Epoch: 1, train for the 144-th batch, train loss: 0.2596157491207123:  36%|███▉       | 144/397 [00:44<01:11,  3.56it/s]Epoch: 1, train for the 145-th batch, train loss: 0.44428327679634094:  36%|███▋      | 144/397 [00:44<01:11,  3.56it/s]Epoch: 1, train for the 145-th batch, train loss: 0.44428327679634094:  37%|███▋      | 145/397 [00:44<01:11,  3.53it/s]Epoch: 1, train for the 146-th batch, train loss: 0.40574848651885986:  37%|███▋      | 145/397 [00:45<01:11,  3.53it/s]Epoch: 1, train for the 146-th batch, train loss: 0.40574848651885986:  37%|███▋      | 146/397 [00:45<01:12,  3.47it/s]Epoch: 1, train for the 147-th batch, train loss: 0.3583172857761383:  37%|████       | 146/397 [00:45<01:12,  3.47it/s]Epoch: 1, train for the 147-th batch, train loss: 0.3583172857761383:  37%|████       | 147/397 [00:45<01:13,  3.41it/s]Epoch: 1, train for the 148-th batch, train loss: 0.26924094557762146:  37%|███▋      | 147/397 [00:45<01:13,  3.41it/s]Epoch: 1, train for the 148-th batch, train loss: 0.26924094557762146:  37%|███▋      | 148/397 [00:45<01:12,  3.45it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3174494206905365:  37%|████       | 148/397 [00:46<01:12,  3.45it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3174494206905365:  38%|████▏      | 149/397 [00:46<01:12,  3.43it/s]Epoch: 1, train for the 149-th batch, train loss: 0.3174494206905365:  38%|████▏      | 149/397 [00:46<01:16,  3.23it/s]
Traceback (most recent call last):
  File "/home2/hmnshpl/projects/DyGLib/train_link_prediction.py", line 202, in <module>
    model[0].compute_src_dst_node_temporal_embeddings(src_node_ids=batch_src_node_ids,
  File "/home2/hmnshpl/projects/DyGLib/models/MemoryModel.py", line 108, in compute_src_dst_node_temporal_embeddings
    updated_node_memories, updated_node_last_updated_times = self.get_updated_memories(node_ids=np.array(range(self.num_nodes)),
                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/hmnshpl/projects/DyGLib/models/MemoryModel.py", line 182, in get_updated_memories
    unique_node_ids, unique_node_messages, unique_node_timestamps = self.message_aggregator.aggregate_messages(node_ids=node_ids,
                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/hmnshpl/projects/DyGLib/models/MemoryModel.py", line 296, in aggregate_messages
    unique_node_messages = torch.stack(unique_node_messages, dim=0) if len(unique_node_messages) > 0 else torch.Tensor([])
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
